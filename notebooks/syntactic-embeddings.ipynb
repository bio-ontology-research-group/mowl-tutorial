{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mowl-borg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0966f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Syntactic embeddings of ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc192f",
   "metadata": {
    "cell_marker": "r\"\"\",\n\"\"\""
   },
   "source": [
    "Syntactic embeddings embedding uses the syntax of axioms to generate sentences out of them. mOWL provides methods to generate text sentences from the axioms and/or the annotations in the ontology. The syntax chosen to generate the sentences is [Manchester Syntax](https://www.w3.org/2007/OWL/draft/ED-owl2-manchester-syntax-20081128/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62091ab",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import mowl\n",
    "mowl.init_jvm(\"10g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e763e1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We import our `Family Ontology` and the method `extract_axiom_corpus`, which extracts the axioms from the ontology and generates sentences in *Manchester Syntax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962f6bf",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mowl.corpus import extract_axiom_corpus\n",
    "from mowl.datasets import PathDataset\n",
    "dataset = PathDataset(\"data/family.owl\")\n",
    "corpus = extract_axiom_corpus(dataset.ontology)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe542cb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's see the corpus generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c534b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for s in corpus[:10]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60c9d3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "Now it is possible to input this corpus in a model like Word2Vec, which will generate numerical representations for our vocabulary. We will use the `gensim` library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b807e0",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [s.split(\" \") for s in corpus]\n",
    "w2v = Word2Vec(sentences, epochs=200, vector_size = 50, min_count = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76928c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Finally, we can provide a visual representation of the entities. We will use a modified version of TSNE, which is implemented in mOWL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.tsne import TSNE\n",
    "\n",
    "vectors = w2v.wv\n",
    "vocab_dict = vectors.key_to_index\n",
    "name_to_label = {c: c.split(\"/\")[-1] for c in vocab_dict if str(c).startswith(\"http://\")}\n",
    "name_to_emb = {c: vectors[[c]][0] for c in name_to_label}\n",
    "\n",
    "tsne = TSNE(name_to_emb, name_to_label)\n",
    "tsne.generate_points(500, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.show(thickness=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ea93b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data augmentation via reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0398be",
   "metadata": {},
   "source": [
    "We can generate more axioms by performing reasoning over the current ontology. mOWL provides access to ELK and Hermit reasoners. Those reasoners can be accessed using the OWLAPI directly or using the `MOWLReasoner` wrapper class that provides some shortcuts to reasoner methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca354fd",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mowl.reasoning.base import MOWLReasoner\n",
    "from org.semanticweb.HermiT import Reasoner\n",
    "\n",
    "reasoner = Reasoner.ReasonerFactory().createReasoner(dataset.ontology)\n",
    "reasoner.precomputeInferences()\n",
    "\n",
    "mowl_reasoner = MOWLReasoner(reasoner)\n",
    "classes_to_infer_over = list(dataset.ontology.getClassesInSignature())\n",
    "\n",
    "subclass_axioms = mowl_reasoner.infer_subclass_axioms(classes_to_infer_over)\n",
    "equivalence_axioms = mowl_reasoner.infer_equivalent_class_axioms(classes_to_infer_over)\n",
    "disjointness_axioms = mowl_reasoner.infer_disjoint_class_axioms(classes_to_infer_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a517e3f",
   "metadata": {},
   "source": [
    "Once the axioms were generated, it is time to add them to the ontology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f15c0f",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mowl.owlapi import OWLAPIAdapter\n",
    "\n",
    "manager = OWLAPIAdapter().owl_manager\n",
    "\n",
    "for ax in subclass_axioms:\n",
    "    manager.addAxiom(dataset.ontology, ax)\n",
    "for ax in equivalence_axioms:\n",
    "    manager.addAxiom(dataset.ontology, ax)\n",
    "for ax in disjointness_axioms:\n",
    "    manager.addAxiom(dataset.ontology, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a200c",
   "metadata": {},
   "source": [
    "Then we can do the embedding process with the updated ontology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379f927",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "corpus = extract_axiom_corpus(dataset.ontology)\n",
    "print(f\"The inferred ontology contains {len(corpus)} axioms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [str(s).split(\" \") for s in corpus]\n",
    "sentences = [[w.replace(\",\", \"\") for w in s] for s in sentences]\n",
    "w2v = Word2Vec(sentences, epochs=200, vector_size = 50, min_count = 0)\n",
    "\n",
    "vectors = w2v.wv\n",
    "vocab_dict = vectors.key_to_index\n",
    "name_to_label = {c: c.split(\"/\")[-1] for c in vocab_dict if str(c).startswith(\"http://\")}\n",
    "name_to_emb = {c: vectors[[c]][0] for c in name_to_label}\n",
    "\n",
    "tsne = TSNE(name_to_emb, name_to_label)\n",
    "tsne.generate_points(500, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff503d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.show(thickness=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8620d",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}