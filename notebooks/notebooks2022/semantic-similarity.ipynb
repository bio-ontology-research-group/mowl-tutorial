{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the MOWL Tutorial hand-on session. \n",
    "\n",
    "Here, we will demonstrate how to use ontologies through the OWLAPI, classify ontologies, reason over them, and compute semantic similarity. In the second part, we use more recent machine learning methods with ontologies.\n",
    "\n",
    "To run a piece of code, just go to the box, and either press Shift+Enter, or click on the \"run cell\" button in the menu bar.\n",
    "\n",
    "The first cells will download all the Java libraries we need.\n",
    "\n",
    "It may take a while to download all the libraries, so you may want to get a coffee while it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4144283315.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_131282/4144283315.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    // @NotebookService nb\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!groovy\n",
    "// @NotebookService nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Download dependencies using Groovy's Grape system\n",
    "import groovy.grape.Grape\n",
    "Grape.grab(group:'org.semanticweb.elk', module:'elk-owlapi', version:'0.4.3')\n",
    "Grape.grab(group:'net.sourceforge.owlapi', module:'owlapi-api', version:'4.5.20')\n",
    "Grape.grab(group:'net.sourceforge.owlapi', module:'owlapi-apibinding', version:'4.5.20')\n",
    "Grape.grab(group:'net.sourceforge.owlapi', module:'owlapi-impl', version:'4.5.20')\n",
    "Grape.grab(group:'com.github.sharispe', module:'slib-sml', version:'0.9.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show some basic operations on ontologies. We need to import lots of classes from the OWLAPI, create an OWL ontology manager, load the ontology, and start exploring a bit.\n",
    "\n",
    "The code below may need to be modified a bit. In the line `ont = manager.loadOntologyFromOntologyDocument(new IRI(\"file:merged-phenomenet.owl\"))`, please change the path to the ontology you want to use. For example, if you want to use the Human Phenotype Ontology directly from the OBO PURL, replace `file:merged-phenomenet.owl` with `https://purl.obolibrary.org/obo/hp.owl`. You can also use the .obo version of the ontologies.\n",
    "\n",
    "__If you have memory problems:__ The ontology we use here is rather large and requires around 10GB of memory to load (and later to classify). If you have problems loading this ontology, you may want to use an .obo file instead of .owl, which may save some memory (but you may not have access to all the axioms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// opens ontology file and prints the number of classes\n",
    "\n",
    "// import what we need\n",
    "import java.net.*\n",
    "import org.semanticweb.owlapi.model.parameters.*\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerFactory;\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerConfiguration\n",
    "import org.semanticweb.elk.reasoner.config.*\n",
    "import org.semanticweb.owlapi.apibinding.OWLManager;\n",
    "import org.semanticweb.owlapi.reasoner.*\n",
    "import org.semanticweb.owlapi.reasoner.structural.StructuralReasoner\n",
    "import org.semanticweb.owlapi.vocab.OWLRDFVocabulary;\n",
    "import org.semanticweb.owlapi.model.*;\n",
    "import org.semanticweb.owlapi.io.*;\n",
    "import org.semanticweb.owlapi.owllink.*;\n",
    "import org.semanticweb.owlapi.util.*;\n",
    "import org.semanticweb.owlapi.search.*;\n",
    "import org.semanticweb.owlapi.manchestersyntax.renderer.*;\n",
    "import org.semanticweb.owlapi.reasoner.structural.*\n",
    " \n",
    "// Let's load an ontology and output the number of classes\n",
    "// Create the ontology manager\n",
    "manager = OWLManager.createOWLOntologyManager()\n",
    "// create data factory (to create axioms, classes, etc.)\n",
    "fac = manager.getOWLDataFactory()\n",
    "// Load the latest version of the PhenomeNET Ontology; for more information about this ontology, see http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005500\n",
    "\n",
    "ont = manager.loadOntologyFromOntologyDocument(new IRI(\"file:merged-phenomenet.owl\"))\n",
    "\n",
    "// Use the next line instead if you have memory problems loading the ontology above (comment out the line above and uncomment this line):\n",
    "//ont = manager.loadOntologyFromOntologyDocument(new IRI(\"file:phenomenet-inferred.owl\"))\n",
    "\n",
    "ont.getClassesInSignature(true).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning with ontologies\n",
    "\n",
    "Reasoning means we use inference rules that use the axioms within the ontology to extract new knowledge (which may or may not be explicitly stated). The most basic operation an OWL reasoner does is to perform _classification_, i.e., it identifies for each class its sub- and super-classes. OWL reasoners can also be used to query ontologies, and find, for example, sub-classes of named classes or complex class descriptions.\n",
    "\n",
    "Reasoning in OWL may be very complex. For example, automated reasoning in OWL 2 is 2NEXPTIME-complete, and will therefore not scale well on arbitrary ontologies. Fortunately, many optimized reasoners exist, so the theoretical limitations are not always a barrier in practise. Nevertheless, OWL 2 comes with several sub-langes (OWL 2 EL, QL, and RL) which guarantee more efficient automated reasoning. In particular the OWL 2 EL language profile is widely used in biological and biomedical ontologies.\n",
    "\n",
    "In the first box below, we will just create a new OWL reasoner object (using the OWL 2 EL reasoner [ELK](https://github.com/liveontologies/elk-reasoner)). In the second box, we will use this reasoner to find all the subclasses of the \"Mode of inheritance\" class in the Human Phenotype Ontology, and output the classes and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import what we need\n",
    "import java.net.*\n",
    "import org.semanticweb.owlapi.model.parameters.*\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerFactory;\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerConfiguration\n",
    "import org.semanticweb.elk.reasoner.config.*\n",
    "import org.semanticweb.owlapi.apibinding.OWLManager;\n",
    "import org.semanticweb.owlapi.reasoner.*\n",
    "import org.semanticweb.owlapi.reasoner.structural.StructuralReasoner\n",
    "import org.semanticweb.owlapi.vocab.OWLRDFVocabulary;\n",
    "import org.semanticweb.owlapi.model.*;\n",
    "import org.semanticweb.owlapi.io.*;\n",
    "import org.semanticweb.owlapi.owllink.*;\n",
    "import org.semanticweb.owlapi.util.*;\n",
    "import org.semanticweb.owlapi.search.*;\n",
    "import org.semanticweb.owlapi.manchestersyntax.renderer.*;\n",
    "import org.semanticweb.owlapi.reasoner.structural.*\n",
    "\n",
    "ConsoleProgressMonitor progressMonitor = new ConsoleProgressMonitor()\n",
    "OWLReasonerConfiguration config = new SimpleConfiguration(progressMonitor)\n",
    "ElkReasonerFactory f1 = new ElkReasonerFactory()\n",
    "reasoner = f1.createReasoner(ont,config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2372109967.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_131282/2372109967.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    // import what we need\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "// import what we need\n",
    "import java.net.*\n",
    "import org.semanticweb.owlapi.model.parameters.*\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerFactory;\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerConfiguration\n",
    "import org.semanticweb.elk.reasoner.config.*\n",
    "import org.semanticweb.owlapi.apibinding.OWLManager;\n",
    "import org.semanticweb.owlapi.reasoner.*\n",
    "import org.semanticweb.owlapi.reasoner.structural.StructuralReasoner\n",
    "import org.semanticweb.owlapi.vocab.OWLRDFVocabulary;\n",
    "import org.semanticweb.owlapi.model.*;\n",
    "import org.semanticweb.owlapi.io.*;\n",
    "import org.semanticweb.owlapi.owllink.*;\n",
    "import org.semanticweb.owlapi.util.*;\n",
    "import org.semanticweb.owlapi.search.*;\n",
    "import org.semanticweb.owlapi.manchestersyntax.renderer.*;\n",
    "import org.semanticweb.owlapi.reasoner.structural.*\n",
    "\n",
    "// get all subclasses of \"Mode of inheritance\", including all descendant classes (direct set to \"false\")\n",
    "reasoner.getSubClasses(fac.getOWLClass(IRI.create(\"http://purl.obolibrary.org/obo/HP_0000005\")), false).getFlattened().each { cl ->\n",
    "  def ciri = cl.getIRI()\n",
    "  // now we get the label (rdfs:label) for each of the classes and print to stdout\n",
    "  EntitySearcher.getAnnotations(cl, ont, fac.getRDFSLabel()).each { a ->\n",
    "    OWLAnnotationValue val = a.getValue()\n",
    "    if(val instanceof OWLLiteral) {\n",
    "      def label = ((OWLLiteral)val).getLiteral()\n",
    "      println \"$label ($ciri)\"\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with this code a bit to find subclasses as well as superclasses for \"Mode of inheritance\" and other classes. A class in OWL does not have to have a URI (these are the named classes) but can also be a complex class description; for example, you can query for subclasses of phenotypes affecting some part of the \"heart\" by querying describing the class using quantifiers, relations, conjunction, disjunction, or negation, and retrieve classes that satisfy this description.\n",
    "\n",
    "\n",
    "Next, we will classify the ontology (from our data package) using the ELK reasoner, and store an inferred version of the ontology in the user home directory. The inferred version will be classified, i.e., all implied sub- and super-class relations are inferred and a transitive reduction is performed so that the resulting taxonomy does not contain redundant sub- or super-class relations.\n",
    "\n",
    "If you do not want to store the file in `$HOME/phenomenet-inferred.owl`, change the code below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import what we need\n",
    "import java.net.*\n",
    "import org.semanticweb.owlapi.model.parameters.*\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerFactory;\n",
    "import org.semanticweb.elk.owlapi.ElkReasonerConfiguration\n",
    "import org.semanticweb.elk.reasoner.config.*\n",
    "import org.semanticweb.owlapi.apibinding.OWLManager\n",
    "import org.semanticweb.owlapi.reasoner.*\n",
    "import org.semanticweb.owlapi.reasoner.structural.StructuralReasoner\n",
    "import org.semanticweb.owlapi.vocab.OWLRDFVocabulary\n",
    "import org.semanticweb.owlapi.model.*\n",
    "import org.semanticweb.owlapi.io.*\n",
    "import org.semanticweb.owlapi.owllink.*\n",
    "import org.semanticweb.owlapi.util.*\n",
    "import org.semanticweb.owlapi.search.*\n",
    "import org.semanticweb.owlapi.manchestersyntax.renderer.*\n",
    "import org.semanticweb.owlapi.reasoner.structural.*\n",
    "    \n",
    "// write the inferred version of the ontology to a file (phenomenet-inferred.owl in the user homedir)\n",
    "def homeDir = System.getProperty(\"user.home\")\n",
    "File outfile = new File (homeDir, \"phenomenet-inferred.owl\")\n",
    "def outont = manager.createOntology(new IRI(\"http://aber-owl.net/mowl-tutorial/phenomenet-inferred.owl\"))\n",
    "InferredOntologyGenerator generator = new InferredOntologyGenerator(reasoner, [new InferredSubClassAxiomGenerator()])\n",
    "generator.fillOntology(fac, outont)\n",
    "\n",
    "manager.saveOntology(outont, IRI.create(\"file:\"+outfile.getAbsolutePath()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity\n",
    "\n",
    "We use the [Semantic Measures Library](http://semantic-measures-library.org) to compute semantic similarity over the inferred ontology. We first generate some basic objects we need to define semantic similarity, and populate the semantic graph (used by the Semantic Measures Library) with the inferred taxonomy generated above.\n",
    "\n",
    "Note: while it is possible to compute semantic similarity over the non-inferred version of the ontology, most similarity measures crucially rely on a graph structure that resembles (or approximates) the semantic content of the ontology. Using a non-inferred version will hide this content. We therefore _always_ recommend to apply semantic similarity measures on the fully inferred version of an ontology.\n",
    "\n",
    "First we set up some basic data structures, mainly the graph over which the similarity will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.openrdf.model.vocabulary.*\n",
    "import slib.sglib.io.loader.*\n",
    "import slib.sml.sm.core.metrics.ic.utils.*\n",
    "import slib.sml.sm.core.utils.*\n",
    "import slib.sglib.io.loader.bio.obo.*\n",
    "import org.openrdf.model.URI\n",
    "import slib.graph.algo.extraction.rvf.instances.*\n",
    "import slib.sglib.algo.graph.utils.*\n",
    "import slib.utils.impl.Timer\n",
    "import slib.graph.algo.extraction.utils.*\n",
    "import slib.graph.model.graph.*\n",
    "import slib.graph.model.repo.*\n",
    "import slib.graph.model.impl.graph.memory.*\n",
    "import slib.sml.sm.core.engine.*\n",
    "import slib.graph.io.conf.*\n",
    "import slib.graph.model.impl.graph.elements.*\n",
    "import slib.graph.algo.extraction.rvf.instances.impl.*\n",
    "import slib.graph.model.impl.repo.*\n",
    "import slib.graph.io.util.*\n",
    "import slib.graph.io.loader.*\n",
    "    \n",
    "factory = URIFactoryMemory.getSingleton()\n",
    "URI graph_uri = factory.getURI(\"http://aber-owl.net/mowl-tutorial/\")\n",
    "graph = new GraphMemory(graph_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we populate the graph with the inferred ontology. We use the RDF_XML format which we generated before. There are some warning messages appearing here which can be safely ignored (because the INFO logger seems to write to stderr by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.openrdf.model.vocabulary.*\n",
    "import slib.sglib.io.loader.*\n",
    "import slib.sml.sm.core.metrics.ic.utils.*\n",
    "import slib.sml.sm.core.utils.*\n",
    "import slib.sglib.io.loader.bio.obo.*\n",
    "import org.openrdf.model.URI\n",
    "import slib.graph.algo.extraction.rvf.instances.*\n",
    "import slib.sglib.algo.graph.utils.*\n",
    "import slib.utils.impl.Timer\n",
    "import slib.graph.algo.extraction.utils.*\n",
    "import slib.graph.model.graph.*\n",
    "import slib.graph.model.repo.*\n",
    "import slib.graph.model.impl.graph.memory.*\n",
    "import slib.sml.sm.core.engine.*\n",
    "import slib.graph.io.conf.*\n",
    "import slib.graph.model.impl.graph.elements.*\n",
    "import slib.graph.algo.extraction.rvf.instances.impl.*\n",
    "import slib.graph.model.impl.repo.*\n",
    "import slib.graph.io.util.*\n",
    "import slib.graph.io.loader.*\n",
    "    \n",
    "def homeDir = System.getProperty(\"user.home\")\n",
    "File ontfile = new File (homeDir, \"phenomenet-inferred.owl\")\n",
    "GDataConf graphconf = new GDataConf(GFormat.RDF_XML, ontfile.getCanonicalPath())\n",
    "GraphLoaderGeneric.populate(graphconf, graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, ontologies (or the taxonomies underlying them) have more than one root (below `owl:Thing`). For example, the Gene Ontology has three roots, _Biological process_, _Molecular function_, and _Cellular Component_. To make classes comparable when they do not share a root (necessary for example in path-based similarity measures), we generate a new artificial root and make this the new root of our semantic graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.openrdf.model.vocabulary.*\n",
    "import slib.sglib.io.loader.*\n",
    "import slib.sml.sm.core.metrics.ic.utils.*\n",
    "import slib.sml.sm.core.utils.*\n",
    "import slib.sglib.io.loader.bio.obo.*\n",
    "import org.openrdf.model.URI\n",
    "import slib.graph.algo.extraction.rvf.instances.*\n",
    "import slib.sglib.algo.graph.utils.*\n",
    "import slib.utils.impl.Timer\n",
    "import slib.graph.algo.extraction.utils.*\n",
    "import slib.graph.model.graph.*\n",
    "import slib.graph.model.repo.*\n",
    "import slib.graph.model.impl.graph.memory.*\n",
    "import slib.sml.sm.core.engine.*\n",
    "import slib.graph.io.conf.*\n",
    "import slib.graph.model.impl.graph.elements.*\n",
    "import slib.graph.algo.extraction.rvf.instances.impl.*\n",
    "import slib.graph.model.impl.repo.*\n",
    "import slib.graph.io.util.*\n",
    "import slib.graph.io.loader.*\n",
    "\n",
    "\n",
    "URI virtualRoot = factory.getURI(\"http://aber-owl.net/mowl-tutorial/virtualRoot\")\n",
    "graph.addV(virtualRoot)\n",
    "GAction rooting = new GAction(GActionType.REROOTING)\n",
    "rooting.addParameter(\"root_uri\", virtualRoot.stringValue())\n",
    "GraphActionExecutor.applyAction(factory, rooting, graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we configure the semantic similarity measure we want to use.\n",
    "\n",
    "Because we do not have instances, we use only the ontology structure to determine how specific a class is (`IC_Conf_Topo` below for use of a specificity measure based only on the topology of the semantic graph). We use Resnik's similarity measure (\"similarity of two classes is the specificity value of their most specific common ancestor\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.openrdf.model.vocabulary.*\n",
    "import slib.sglib.io.loader.*\n",
    "import slib.sml.sm.core.metrics.ic.utils.*\n",
    "import slib.sml.sm.core.utils.*\n",
    "import slib.sglib.io.loader.bio.obo.*\n",
    "import org.openrdf.model.URI\n",
    "import slib.graph.algo.extraction.rvf.instances.*\n",
    "import slib.sglib.algo.graph.utils.*\n",
    "import slib.utils.impl.Timer\n",
    "import slib.graph.algo.extraction.utils.*\n",
    "import slib.graph.model.graph.*\n",
    "import slib.graph.model.repo.*\n",
    "import slib.graph.model.impl.graph.memory.*\n",
    "import slib.sml.sm.core.engine.*\n",
    "import slib.graph.io.conf.*\n",
    "import slib.graph.model.impl.graph.elements.*\n",
    "import slib.graph.algo.extraction.rvf.instances.impl.*\n",
    "import slib.graph.model.impl.repo.*\n",
    "import slib.graph.io.util.*\n",
    "import slib.graph.io.loader.*\n",
    "\n",
    "// configure the semantic similarity measure: use Resnik's (extrinsic) information content measure, and Resnik's similarity measure\n",
    "icConf = new IC_Conf_Topo(\"Sanchez\", SMConstants.FLAG_ICI_SANCHEZ_2011)\n",
    "smConfPairwise = new SMconf(\"Resnik\", SMConstants.FLAG_SIM_PAIRWISE_DAG_NODE_RESNIK_1995 )\n",
    "smConfPairwise.setICconf(icConf)\n",
    "\n",
    "// initialize the engine\n",
    "engine = new SM_Engine(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare some classes and determine how similar they are. Feel free to change the code to some new classes and see how the similarity changes.\n",
    "\n",
    "You can use classes from the Human Phenotype Ontology (HPO) and the Mammalian Phenotype Ontology (MPO) for similarity computation between phenotypes. Since we are using the PhenomeNET ontology (see [our paper](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005500)), classes in both ontologies are actually comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// similarity between two classes from HPO\n",
    "cl1 = factory.getURI(\"http://purl.obolibrary.org/obo/HP_0030766\") // Ear pain\n",
    "cl2 = factory.getURI(\"http://purl.obolibrary.org/obo/HP_0012781\") // mid frequency hearing loss\n",
    "println \"Similarity between 'Ear pain' and 'mid frequency hearing loss': \"+engine.compare(smConfPairwise, cl1, cl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// similarity between two classes from HPO\n",
    "cl2 = factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001636\") // tetralogy of fallot\n",
    "println \"Similarity between 'Ear pain' and 'tetralogy of fallot': \"+engine.compare(smConfPairwise, cl1, cl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3752159608.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_131282/3752159608.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    // similarity between two classes, one from HPO and one from MP\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "// similarity between two classes, one from HPO and one from MP\n",
    "cl1 = factory.getURI(\"http://purl.obolibrary.org/obo/MP_0004084\") // abnormal cardiac muscle relaxation\n",
    "cl2 = factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001636\") // tetralogy of fallot\n",
    "println \"Similarity between 'abnormal cardiac muscle relaxation' (MP) and 'tetralogy of fallot' (HP): \"+engine.compare(smConfPairwise, cl1, cl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// similarity between two classes, one from HPO and one from MP\n",
    "cl1 = factory.getURI(\"http://purl.obolibrary.org/obo/MP_0010402\") // ventricular septal defect\n",
    "println \"Similarity between 'ventricular septal defect' (MP) and 'tetralogy of fallot' (HP): \"+engine.compare(smConfPairwise, cl1, cl2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only looked at comparisons between two individual classes. This may be nice if we just want to understand how the classes in an ontology are related, but usually we want to compare _things that are annotated with classes_ from an ontology.\n",
    "\n",
    "We now download mouse phenotypes resulting from single gene knockouts (note: replace `http://www.informatics.jax.org/downloads/reports/MGI_GenePheno.rpt` with a `file:...` URL in case of slow Internet connection; you can find the file `MGI_GenePheno.rpt` in our data package), and add each gene as an instance of its phenotype classes (using an `rdf:type` edge). \n",
    "\n",
    "Important: this is __not__ the right way to do this in an ontology but is an artifact of the Semantic Measures Library. When building an ontology, a _gene_ is not an _instance_ of a phenotype, but will be related to the phenotype through other kinds of relations. In the Semantic Measures Library, we will treat them as instances, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.openrdf.model.vocabulary.*\n",
    "import slib.sglib.io.loader.*\n",
    "import slib.sml.sm.core.metrics.ic.utils.*\n",
    "import slib.sml.sm.core.utils.*\n",
    "import slib.sglib.io.loader.bio.obo.*\n",
    "import org.openrdf.model.URI\n",
    "import slib.graph.algo.extraction.rvf.instances.*\n",
    "import slib.sglib.algo.graph.utils.*\n",
    "import slib.utils.impl.Timer\n",
    "import slib.graph.algo.extraction.utils.*\n",
    "import slib.graph.model.graph.*\n",
    "import slib.graph.model.repo.*\n",
    "import slib.graph.model.impl.graph.memory.*\n",
    "import slib.sml.sm.core.engine.*\n",
    "import slib.graph.io.conf.*\n",
    "import slib.graph.model.impl.graph.elements.*\n",
    "import slib.graph.algo.extraction.rvf.instances.impl.*\n",
    "import slib.graph.model.impl.repo.*\n",
    "import slib.graph.io.util.*\n",
    "import slib.graph.io.loader.*\n",
    "\n",
    "// now we download mouse phenotype annotations and add them to our graph as instances (using rdf:type)\n",
    "new URL (\"file:misc/MGI_GenePheno.rpt\").getText().splitEachLine(\"\\t\") { line ->\n",
    "  def geneid = line[6]\n",
    "  def idUri = factory.getURI(\"http://phenomebrowser.net/mowl-tutorial/gene/\"+geneid)\n",
    "  def pheno = line[4].replaceAll(\":\",\"_\")\n",
    "  def phenoUri = factory.getURI(\"http://purl.obolibrary.org/obo/\"+pheno)\n",
    "  Edge e = new Edge(idUri, RDF.TYPE, phenoUri)\n",
    "  graph.addE(e)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the presence of instances, we can change our class specificity measure. We now use Resnik's specificity measure, which is just the information content of a class (i.e., `-log(p(c))` where `p(c)` is the probability to have an instance annotated with the class. This is a specificity measure that relies on instances, and is therefore a _corpus-based_ measure (using `IC_Conf_Corpus` below).\n",
    "\n",
    "We also configure a group-wise measure now, using the Best-Matching Average strategy. This will allow us to compare sets of classes to sets of classes.\n",
    "\n",
    "Finally, the `InstanceAccessor` will allow us to identify all instances of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.openrdf.model.vocabulary.*\n",
    "import slib.sglib.io.loader.*\n",
    "import slib.sml.sm.core.metrics.ic.utils.*\n",
    "import slib.sml.sm.core.utils.*\n",
    "import slib.sglib.io.loader.bio.obo.*\n",
    "import org.openrdf.model.URI\n",
    "import slib.graph.algo.extraction.rvf.instances.*\n",
    "import slib.sglib.algo.graph.utils.*\n",
    "import slib.utils.impl.Timer\n",
    "import slib.graph.algo.extraction.utils.*\n",
    "import slib.graph.model.graph.*\n",
    "import slib.graph.model.repo.*\n",
    "import slib.graph.model.impl.graph.memory.*\n",
    "import slib.sml.sm.core.engine.*\n",
    "import slib.graph.io.conf.*\n",
    "import slib.graph.model.impl.graph.elements.*\n",
    "import slib.graph.algo.extraction.rvf.instances.impl.*\n",
    "import slib.graph.model.impl.repo.*\n",
    "import slib.graph.io.util.*\n",
    "import slib.graph.io.loader.*\n",
    "\n",
    "\n",
    "// now that we have instances/annotations, we switch to Resnik's information content measure\n",
    "icConf = new IC_Conf_Corpus(\"ResnikIC\", SMConstants.FLAG_IC_ANNOT_RESNIK_1995_NORMALIZED)\n",
    "smConfPairwise.setICconf(icConf)\n",
    "// using Best-Matching Average to merge annotations when comparing sets of classes\n",
    "smConfGroupwise = new SMconf(\"BMA\", SMConstants.FLAG_SIM_GROUPWISE_BMA)\n",
    "// and re-initialize the graph\n",
    "engine = new SM_Engine(graph)\n",
    "\n",
    "// we need this to find our instances again\n",
    "ia = new InstanceAccessor_RDF_TYPE(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute again similarity between two classes. Note how the similarity values change. Feel free to experiment with this and compare different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// similarity between two classes, one from HPO and one from MP\n",
    "cl1 = factory.getURI(\"http://purl.obolibrary.org/obo/MP_0004084\") // abnormal cardiac muscle relaxation\n",
    "cl2 = factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001636\") // tetralogy of fallot\n",
    "println \"Similarity (Resnik) between 'abnormal cardiac muscle relaxation' (MP) and 'tetralogy of fallot' (HP): \"+engine.compare(smConfPairwise, cl1, cl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// similarity between two classes, one from HPO and one from MP\n",
    "cl1 = factory.getURI(\"http://purl.obolibrary.org/obo/MP_0010402\") // ventricular septal defect\n",
    "println \"Similarity (Resnik) between 'ventricular septal defect' (MP) and 'tetralogy of fallot' (HP): \"+engine.compare(smConfPairwise, cl1, cl2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the presence of mouse genes as instances in our semantic graph, we can do some more sophisticated analyses. Here, we define a patient through a set of phenotypes (i.e., clinical signs and symptoms). The patient has a rare, heritable form of macular dystrophy without known etiology (see https://www.omim.org/entry/153840). We hypothesize that there may be a mouse knockout that resembles this disease, and possibly the human ortholog of the mouse gene may also be involved in this form of macular dystrophy.\n",
    "\n",
    "We compare our patient to _all_ instances in our graph (i.e., all mouse knockouts) using our measure of phenotypic similarity. We then sort the resulting list by their similarity score in descending order, and output the top 10 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// that's our patient, with a single phenotype of an orphan disease, \"MACULAR DYSTROPHY, VITELLIFORM, 1; VMD1\" (https://www.omim.org/entry/153840)\n",
    "Set patient = [ factory.getURI(\"http://purl.obolibrary.org/obo/HP_0007754\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0007663\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001123\"), \n",
    "  factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000505\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0007677\") ]\n",
    "// we store the similarity values in the results set\n",
    "def results = new LinkedHashSet()\n",
    "engine.getInstances().each { gene ->\n",
    "  def phenoSet = ia.getDirectClass(gene) // gets all the annotations of each instance (gene)\n",
    "  Expando exp = new Expando()\n",
    "  try { // this part might fail if the version of MP we use to compute similarity and the MP used for annotation is different; we ignore all errors for now\n",
    "    exp.sim = engine.compare(smConfGroupwise, smConfPairwise, phenoSet, patient)\n",
    "    exp.gene = gene\n",
    "    results.add(exp)\n",
    "  } catch (Exception E) {}\n",
    "}\n",
    "// sorting results by similarity (highest first) and output the top 10 predictions\n",
    "println \"Highest ranking genes for patient 1 (macular dystrophy): \"\n",
    "results.sort { it.sim }.reverse()[0..10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this with another patient, this time with Wiskott-Aldrich Syndrome (https://www.omim.org/entry/301000). \n",
    "Wiskott-Aldrich Syndrome is a rare genetically-based disease in which the WAS gene is known to be causally involved. In the mouse, there is also a FOXP3 model that is used as a disease model for Wiskott-Aldrich syndrome (http://www.informatics.jax.org/allele/MGI:1857034)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// another patient with Wiskott-Aldrich Syndrome\n",
    "Set patient = [ factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000112\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000225\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000246\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000388\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000421\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000964\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000967\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0000979\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001287\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001419\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001873\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001878\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001888\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001891\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0001983\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002037\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002090\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002248\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002249\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002783\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002788\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002848\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002850\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002963\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0002971\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0003010\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0003212\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0003261\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0005310\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0005537\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0011944\"), factory.getURI(\"http://purl.obolibrary.org/obo/HP_0040184\") ]\n",
    "def results = new LinkedHashSet()\n",
    "engine.getInstances().each { gene ->\n",
    "  def phenoSet = ia.getDirectClass(gene) // gets all the annotations of each instance (gene)\n",
    "  Expando exp = new Expando()\n",
    "  try { // this might fail if the version of MP we use to compute similarity and the MP used for annotation is different; we ignore this for now\n",
    "    exp.sim = engine.compare(smConfGroupwise, smConfPairwise, phenoSet, patient)\n",
    "    exp.gene = gene\n",
    "    results.add(exp)\n",
    "  } catch (Exception E) { }\n",
    "}\n",
    "// sorting results by similarity (highest first) and output the top 10 predictions\n",
    "println \"Highest ranking genes for patient 2 (Wiskott Aldrich Syndrom): \"\n",
    "results.sort { it.sim }.reverse()[0..10] // includes MGI:1891436, FOXP3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
